{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06cee82",
   "metadata": {},
   "source": [
    "### set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf267af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6e861",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95a34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        response.raise_for_status()\n",
    "        return await response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R scraping functions\n",
    "\n",
    "\n",
    "def find_2nd_td_following_certain_td(soup, td_text) -> BeautifulSoup:\n",
    "    # Find the first <td> element that contains td_text\n",
    "    td = soup.find(\"td\", string=re.compile(td_text))\n",
    "    if td:\n",
    "        # Find the next <td> element\n",
    "        next_td = td.find_next(\"td\")\n",
    "        if next_td:\n",
    "            return next_td\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_r_release(soup) -> str:\n",
    "    td = find_2nd_td_following_certain_td(soup, \"binaries\")\n",
    "    found = False\n",
    "    for item in td:\n",
    "        if found:\n",
    "            return item[\"href\"]\n",
    "        if item.name is None and \"r-release:\" in item.strip():\n",
    "            found = True\n",
    "\n",
    "\n",
    "def find_all_imports_refs(soup) -> list:\n",
    "    td = find_2nd_td_following_certain_td(soup, \"Imports:\")\n",
    "    results = []\n",
    "    if td:\n",
    "        a_tags = td.find_all(\"a\")\n",
    "        if a_tags:\n",
    "            for a in a_tags:\n",
    "                results.append(a[\"href\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4308df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R scraping functions\n",
    "\n",
    "ROOT_URL = \"https://cran.r-project.org\"\n",
    "\n",
    "\n",
    "def bin_link_scrap(page) -> str:\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    relative_link = find_r_release(soup)\n",
    "    if relative_link:\n",
    "        new_path = [*Path(relative_link).parts[3:]]  # Skips the first three '..'\n",
    "        return \"/\".join([ROOT_URL] + new_path)\n",
    "\n",
    "\n",
    "def imports_link_scrap(page) -> list:\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    imports = find_all_imports_refs(soup)\n",
    "\n",
    "    results = []\n",
    "    for relative_link in imports:\n",
    "        new_path = [*Path(relative_link).parts[1:]]  # Skip the first '..'\n",
    "        results.append(\"/\".join([ROOT_URL, \"web\", \"packages\"] + new_path))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R scraping functions - iterative process\n",
    "\n",
    "\n",
    "async def get_r_bins(session, url, results=None, processed_urls=None) -> tuple:\n",
    "    if results is None:\n",
    "        results = set()\n",
    "    if processed_urls is None:\n",
    "        processed_urls = set()\n",
    "\n",
    "    print(f\"processing {url}\")\n",
    "    page = await fetch(session, url)\n",
    "\n",
    "    bin_link = bin_link_scrap(page)\n",
    "    dependencies = imports_link_scrap(page)\n",
    "\n",
    "    results.add(bin_link)\n",
    "    processed_urls.add(url)\n",
    "\n",
    "    for dep_url in dependencies:\n",
    "        if dep_url not in processed_urls:\n",
    "            results, processed_urls = await get_r_bins(\n",
    "                session, dep_url, results, processed_urls\n",
    "            )\n",
    "\n",
    "    return results, processed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bef6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download functions\n",
    "\n",
    "\n",
    "def write_data_to_file(data, file_path):\n",
    "    if not file_path.exists():\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(data)\n",
    "\n",
    "\n",
    "async def down_load_r_bins(session, urls, zip_path):\n",
    "\n",
    "    async def process_one_file(session, url, zip_path):\n",
    "        name = Path(url).name\n",
    "        if Path(zip_path / name).exists():\n",
    "            print(f\"{name} already exists, skipping download.\")\n",
    "            return\n",
    "\n",
    "        print(f\"downloading {url}\")\n",
    "        data = await fetch(session, url)\n",
    "\n",
    "        file_path = zip_path / name\n",
    "        await asyncio.to_thread(write_data_to_file, data, file_path)\n",
    "\n",
    "    tasks = [process_one_file(session, url, zip_path) for url in urls]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip functions\n",
    "\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "\n",
    "async def process_zip_files(zip_files: list, extract_to: str):\n",
    "    async def process_one_file(zip_path, extract_to):\n",
    "        stem = Path(zip_path).stem\n",
    "        base_name = re.sub(r\"_\\d+(\\.\\d+)*$\", \"\", stem)\n",
    "\n",
    "        if Path(extract_to / base_name).exists():\n",
    "            print(f\"{base_name} already exists, skipping unzipping.\")\n",
    "            return\n",
    "\n",
    "        print(f\"unzipping {zip_path}\")\n",
    "        await asyncio.to_thread(unzip_file, zip_path, extract_to)\n",
    "\n",
    "    tasks = []\n",
    "    for zip_file in zip_files:\n",
    "        tasks.append(process_one_file(zip_file, extract_to))\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3bf89",
   "metadata": {},
   "source": [
    "### process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f87d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up and clean up the folders\n",
    "folders = [Path(\".\") / \"zip\", Path(\".\") / \"unzip\"]\n",
    "import shutil\n",
    "\n",
    "for folder in folders:\n",
    "    if not folder.exists():\n",
    "        folder.mkdir()\n",
    "    else:\n",
    "        shutil.rmtree(folder)\n",
    "        folder.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8bd0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cran.r-project.org/web/packages/Hmisc/index.html\"\n",
    "zip_path = Path(\".\") / \"zip\"\n",
    "unzip_path = Path(\".\") / \"unzip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9622843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing https://cran.r-project.org/web/packages/Hmisc/index.html\n",
      "processing https://cran.r-project.org/web/packages/ggplot2/index.html\n",
      "processing https://cran.r-project.org/web/packages/cli/index.html\n",
      "processing https://cran.r-project.org/web/packages/glue/index.html\n",
      "processing https://cran.r-project.org/web/packages/gtable/index.html\n",
      "processing https://cran.r-project.org/web/packages/lifecycle/index.html\n",
      "processing https://cran.r-project.org/web/packages/rlang/index.html\n",
      "processing https://cran.r-project.org/web/packages/isoband/index.html\n",
      "processing https://cran.r-project.org/web/packages/MASS/index.html\n",
      "processing https://cran.r-project.org/web/packages/mgcv/index.html\n",
      "processing https://cran.r-project.org/web/packages/Matrix/index.html\n",
      "processing https://cran.r-project.org/web/packages/lattice/index.html\n",
      "processing https://cran.r-project.org/web/packages/scales/index.html\n",
      "processing https://cran.r-project.org/web/packages/farver/index.html\n",
      "processing https://cran.r-project.org/web/packages/labeling/index.html\n",
      "processing https://cran.r-project.org/web/packages/R6/index.html\n",
      "processing https://cran.r-project.org/web/packages/RColorBrewer/index.html\n",
      "processing https://cran.r-project.org/web/packages/viridisLite/index.html\n",
      "processing https://cran.r-project.org/web/packages/tibble/index.html\n",
      "processing https://cran.r-project.org/web/packages/fansi/index.html\n",
      "processing https://cran.r-project.org/web/packages/magrittr/index.html\n",
      "processing https://cran.r-project.org/web/packages/pillar/index.html\n",
      "processing https://cran.r-project.org/web/packages/utf8/index.html\n",
      "processing https://cran.r-project.org/web/packages/vctrs/index.html\n",
      "processing https://cran.r-project.org/web/packages/pkgconfig/index.html\n",
      "processing https://cran.r-project.org/web/packages/withr/index.html\n",
      "processing https://cran.r-project.org/web/packages/cluster/index.html\n",
      "processing https://cran.r-project.org/web/packages/rpart/index.html\n",
      "processing https://cran.r-project.org/web/packages/nnet/index.html\n",
      "processing https://cran.r-project.org/web/packages/foreign/index.html\n",
      "processing https://cran.r-project.org/web/packages/gridExtra/index.html\n",
      "processing https://cran.r-project.org/web/packages/data.table/index.html\n",
      "processing https://cran.r-project.org/web/packages/htmlTable/index.html\n",
      "processing https://cran.r-project.org/web/packages/stringr/index.html\n",
      "processing https://cran.r-project.org/web/packages/stringi/index.html\n",
      "processing https://cran.r-project.org/web/packages/knitr/index.html\n",
      "processing https://cran.r-project.org/web/packages/evaluate/index.html\n",
      "processing https://cran.r-project.org/web/packages/highr/index.html\n",
      "processing https://cran.r-project.org/web/packages/xfun/index.html\n",
      "processing https://cran.r-project.org/web/packages/yaml/index.html\n",
      "processing https://cran.r-project.org/web/packages/checkmate/index.html\n",
      "processing https://cran.r-project.org/web/packages/backports/index.html\n",
      "processing https://cran.r-project.org/web/packages/htmlwidgets/index.html\n",
      "processing https://cran.r-project.org/web/packages/htmltools/index.html\n",
      "processing https://cran.r-project.org/web/packages/base64enc/index.html\n",
      "processing https://cran.r-project.org/web/packages/digest/index.html\n",
      "processing https://cran.r-project.org/web/packages/fastmap/index.html\n",
      "processing https://cran.r-project.org/web/packages/jsonlite/index.html\n",
      "processing https://cran.r-project.org/web/packages/rmarkdown/index.html\n",
      "processing https://cran.r-project.org/web/packages/bslib/index.html\n",
      "processing https://cran.r-project.org/web/packages/cachem/index.html\n",
      "processing https://cran.r-project.org/web/packages/jquerylib/index.html\n",
      "processing https://cran.r-project.org/web/packages/memoise/index.html\n",
      "processing https://cran.r-project.org/web/packages/mime/index.html\n",
      "processing https://cran.r-project.org/web/packages/sass/index.html\n",
      "processing https://cran.r-project.org/web/packages/fs/index.html\n",
      "processing https://cran.r-project.org/web/packages/rappdirs/index.html\n",
      "processing https://cran.r-project.org/web/packages/fontawesome/index.html\n",
      "processing https://cran.r-project.org/web/packages/tinytex/index.html\n",
      "processing https://cran.r-project.org/web/packages/rstudioapi/index.html\n",
      "processing https://cran.r-project.org/web/packages/viridis/index.html\n",
      "processing https://cran.r-project.org/web/packages/colorspace/index.html\n",
      "processing https://cran.r-project.org/web/packages/Formula/index.html\n",
      "len(results) = 63\n"
     ]
    }
   ],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    results, processed_urls = await get_r_bins(session, url)\n",
    "\n",
    "print(f\"{len(results) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55561c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fansi_1.0.6.zip already exists, skipping download.\n",
      "bslib_0.9.0.zip already exists, skipping download.\n",
      "ggplot2_3.5.2.zip already exists, skipping download.\n",
      "evaluate_1.0.3.zip already exists, skipping download.\n",
      "rappdirs_0.3.3.zip already exists, skipping download.\n",
      "mgcv_1.9-3.zip already exists, skipping download.\n",
      "nnet_7.3-20.zip already exists, skipping download.\n",
      "utf8_1.2.5.zip already exists, skipping download.\n",
      "yaml_2.3.10.zip already exists, skipping download.\n",
      "cachem_1.1.0.zip already exists, skipping download.\n",
      "jquerylib_0.1.4.zip already exists, skipping download.\n",
      "stringi_1.8.7.zip already exists, skipping download.\n",
      "xfun_0.52.zip already exists, skipping download.\n",
      "glue_1.8.0.zip already exists, skipping download.\n",
      "labeling_0.4.3.zip already exists, skipping download.\n",
      "htmlwidgets_1.6.4.zip already exists, skipping download.\n",
      "jsonlite_2.0.0.zip already exists, skipping download.\n",
      "cli_3.6.5.zip already exists, skipping download.\n",
      "RColorBrewer_1.1-3.zip already exists, skipping download.\n",
      "backports_1.5.0.zip already exists, skipping download.\n",
      "fontawesome_0.5.3.zip already exists, skipping download.\n",
      "farver_2.1.2.zip already exists, skipping download.\n",
      "gtable_0.3.6.zip already exists, skipping download.\n",
      "fastmap_1.2.0.zip already exists, skipping download.\n",
      "stringr_1.5.1.zip already exists, skipping download.\n",
      "memoise_2.0.1.zip already exists, skipping download.\n",
      "fs_1.6.6.zip already exists, skipping download.\n",
      "tinytex_0.57.zip already exists, skipping download.\n",
      "knitr_1.50.zip already exists, skipping download.\n",
      "R6_2.6.1.zip already exists, skipping download.\n",
      "rmarkdown_2.29.zip already exists, skipping download.\n",
      "viridis_0.6.5.zip already exists, skipping download.\n",
      "pillar_1.10.2.zip already exists, skipping download.\n",
      "foreign_0.8-90.zip already exists, skipping download.\n",
      "lifecycle_1.0.4.zip already exists, skipping download.\n",
      "cluster_2.1.8.1.zip already exists, skipping download.\n",
      "htmltools_0.5.8.1.zip already exists, skipping download.\n",
      "digest_0.6.37.zip already exists, skipping download.\n",
      "htmlTable_2.4.3.zip already exists, skipping download.\n",
      "checkmate_2.3.2.zip already exists, skipping download.\n",
      "mime_0.13.zip already exists, skipping download.\n",
      "rlang_1.1.6.zip already exists, skipping download.\n",
      "pkgconfig_2.0.3.zip already exists, skipping download.\n",
      "data.table_1.17.2.zip already exists, skipping download.\n",
      "base64enc_0.1-3.zip already exists, skipping download.\n",
      "colorspace_2.1-1.zip already exists, skipping download.\n",
      "rpart_4.1.24.zip already exists, skipping download.\n",
      "Matrix_1.7-3.zip already exists, skipping download.\n",
      "rstudioapi_0.17.1.zip already exists, skipping download.\n",
      "isoband_0.2.7.zip already exists, skipping download.\n",
      "highr_0.11.zip already exists, skipping download.\n",
      "sass_0.4.10.zip already exists, skipping download.\n",
      "gridExtra_2.3.zip already exists, skipping download.\n",
      "viridisLite_0.4.2.zip already exists, skipping download.\n",
      "withr_3.0.2.zip already exists, skipping download.\n",
      "tibble_3.2.1.zip already exists, skipping download.\n",
      "MASS_7.3-65.zip already exists, skipping download.\n",
      "Hmisc_5.2-3.zip already exists, skipping download.\n",
      "vctrs_0.6.5.zip already exists, skipping download.\n",
      "lattice_0.22-7.zip already exists, skipping download.\n",
      "scales_1.4.0.zip already exists, skipping download.\n",
      "Formula_1.2-5.zip already exists, skipping download.\n",
      "magrittr_2.0.3.zip already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# throttle the download\n",
    "number_of_simultaneous_downloads = 4\n",
    "list_of_urls = list(results)\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    for i in range(0, len(results), number_of_simultaneous_downloads):\n",
    "        urls_to_download = list_of_urls[i : i + number_of_simultaneous_downloads]\n",
    "        await down_load_r_bins(session, urls_to_download, zip_path)\n",
    "        await asyncio.sleep(1)  # Sleep for 1 second between batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35429b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backports already exists, skipping unzipping.\n",
      "unzipping zip\\base64enc_0.1-3.zip\n",
      "bslib already exists, skipping unzipping.\n",
      "cachem already exists, skipping unzipping.\n",
      "checkmate already exists, skipping unzipping.\n",
      "cli already exists, skipping unzipping.\n",
      "cluster already exists, skipping unzipping.\n",
      "unzipping zip\\colorspace_2.1-1.zip\n",
      "unzipping zip\\data.table_1.17.2.zip\n",
      "digest already exists, skipping unzipping.\n",
      "evaluate already exists, skipping unzipping.\n",
      "fansi already exists, skipping unzipping.\n",
      "unzipping zip\\farver_2.1.2.zip\n",
      "unzipping zip\\fastmap_1.2.0.zip\n",
      "fontawesome already exists, skipping unzipping.\n",
      "unzipping zip\\foreign_0.8-90.zip\n",
      "unzipping zip\\Formula_1.2-5.zip\n",
      "fs already exists, skipping unzipping.\n",
      "ggplot2 already exists, skipping unzipping.\n",
      "glue already exists, skipping unzipping.\n",
      "gridExtra already exists, skipping unzipping.\n",
      "gtable already exists, skipping unzipping.\n",
      "highr already exists, skipping unzipping.\n",
      "unzipping zip\\Hmisc_5.2-3.zip\n",
      "unzipping zip\\htmlTable_2.4.3.zip\n",
      "htmltools already exists, skipping unzipping.\n",
      "htmlwidgets already exists, skipping unzipping.\n",
      "isoband already exists, skipping unzipping.\n",
      "jquerylib already exists, skipping unzipping.\n",
      "jsonlite already exists, skipping unzipping.\n",
      "knitr already exists, skipping unzipping.\n",
      "labeling already exists, skipping unzipping.\n",
      "unzipping zip\\lattice_0.22-7.zip\n",
      "lifecycle already exists, skipping unzipping.\n",
      "magrittr already exists, skipping unzipping.\n",
      "unzipping zip\\MASS_7.3-65.zip\n",
      "unzipping zip\\Matrix_1.7-3.zip\n",
      "unzipping zip\\memoise_2.0.1.zip\n",
      "unzipping zip\\mgcv_1.9-3.zip\n",
      "mime already exists, skipping unzipping.\n",
      "unzipping zip\\nnet_7.3-20.zip\n",
      "pillar already exists, skipping unzipping.\n",
      "pkgconfig already exists, skipping unzipping.\n",
      "R6 already exists, skipping unzipping.\n",
      "rappdirs already exists, skipping unzipping.\n",
      "unzipping zip\\RColorBrewer_1.1-3.zip\n",
      "rlang already exists, skipping unzipping.\n",
      "rmarkdown already exists, skipping unzipping.\n",
      "rpart already exists, skipping unzipping.\n",
      "unzipping zip\\rstudioapi_0.17.1.zip\n",
      "unzipping zip\\sass_0.4.10.zip\n",
      "scales already exists, skipping unzipping.\n",
      "unzipping zip\\stringi_1.8.7.zip\n",
      "stringr already exists, skipping unzipping.\n",
      "tibble already exists, skipping unzipping.\n",
      "unzipping zip\\tinytex_0.57.zip\n",
      "unzipping zip\\utf8_1.2.5.zip\n",
      "vctrs already exists, skipping unzipping.\n",
      "viridisLite already exists, skipping unzipping.\n",
      "unzipping zip\\viridis_0.6.5.zip\n",
      "withr already exists, skipping unzipping.\n",
      "unzipping zip\\xfun_0.52.zip\n",
      "unzipping zip\\yaml_2.3.10.zip\n"
     ]
    }
   ],
   "source": [
    "zip_files = list(zip_path.glob(\"*.zip\"))\n",
    "await process_zip_files(zip_files, unzip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fee0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up and clean up the folders\n",
    "folders = [Path(\".\") / \"zip\", Path(\".\") / \"unzip\"]\n",
    "import shutil\n",
    "\n",
    "for folder in folders:\n",
    "    if not folder.exists():\n",
    "        folder.mkdir()\n",
    "    else:\n",
    "        shutil.rmtree(folder)\n",
    "        folder.mkdir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
