{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter imports\n",
    "import json\n",
    "\n",
    "# TODO: add examples of heuristic checks and simple usability metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb42d22",
   "metadata": {},
   "source": [
    "## Usability Heuristics Checklist\n",
    "- Keep interfaces consistent, observable, and forgiving.\n",
    "- Track heuristic scores to prioritize fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristics = {\n",
    "    \"visibility\": 4,  # feedback/affordances\n",
    "    \"consistency\": 4,\n",
    "    \"error_prevention\": 4,\n",
    "    \"help_docs\": 3,\n",
    "}\n",
    "\n",
    "def score(responses: dict[str, int]):\n",
    "    keys = heuristics.keys()\n",
    "    total = sum(responses.get(k, 0) for k in keys)\n",
    "    return round(total / len(keys), 2)\n",
    "\n",
    "sample = {\"visibility\": 3, \"consistency\": 4, \"error_prevention\": 2, \"help_docs\": 3}\n",
    "score(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ea6b6",
   "metadata": {},
   "source": [
    "Interpretation: average ≥3 is acceptable, <3 needs UX fixes. Track per-heuristic scores across releases to see regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcca7a",
   "metadata": {},
   "source": [
    "## Accessibility Quick Wins\n",
    "- Ensure color contrast and focus states; support keyboard-only navigation.\n",
    "- Provide aria-labels for controls; avoid conveying meaning by color alone.\n",
    "- Test with screen readers and reduced-motion settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_low_scores(responses: dict[str, int], threshold=3):\n",
    "    return {k: v for k, v in responses.items() if v < threshold}\n",
    "\n",
    "flag_low_scores(sample, threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d4414",
   "metadata": {},
   "source": [
    "## Research & Measurement\n",
    "- Use lightweight usability tests (5–7 users) to surface most severe issues early.\n",
    "- Track task success rate, time-on-task, error rate, and SUS score to quantify usability.\n",
    "- Map critical flows (signup, checkout) and instrument them; fix friction where drop-off is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f32976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate usability session metrics\n",
    "sessions = [\n",
    "    {\"success\": True, \"time_sec\": 42, \"errors\": 0},\n",
    "    {\"success\": False, \"time_sec\": 75, \"errors\": 2},\n",
    "    {\"success\": True, \"time_sec\": 55, \"errors\": 1},\n",
    "]\n",
    "\n",
    "success_rate = sum(s[\"success\"] for s in sessions) / len(sessions)\n",
    "avg_time = round(sum(s[\"time_sec\"] for s in sessions) / len(sessions), 1)\n",
    "avg_errors = round(sum(s[\"errors\"] for s in sessions) / len(sessions), 2)\n",
    "\n",
    "{\"success_rate\": success_rate, \"avg_time\": avg_time, \"avg_errors\": avg_errors}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
