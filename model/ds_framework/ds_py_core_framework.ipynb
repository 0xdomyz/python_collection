{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Python for Data Science in Credit Risk Modeling - Core Framework\n",
                "\n",
                "This notebook outlines the key components for implementing credit risk models in commercial banking using Python."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Preparation\n",
                "\n",
                "### Data Acquisition\n",
                "- **Key Libraries**: pandas, sqlalchemy, requests\n",
                "- **Banking Applications**: Loan portfolios, credit bureau data, economic indicators\n",
                "- **Key Skills**: Database connectivity, API integration, file processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Acquisition Example\n",
                "import pandas as pd\n",
                "import sqlalchemy as sa\n",
                "import requests\n",
                "\n",
                "# Database connection example\n",
                "# engine = sa.create_engine('your_database_connection_string')\n",
                "# loan_data = pd.read_sql('SELECT * FROM loan_portfolio', engine)\n",
                "\n",
                "# File processing example\n",
                "# loan_data = pd.read_csv('loan_portfolio.csv')\n",
                "\n",
                "print(\"Data acquisition libraries loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Cleaning\n",
                "- **Key Libraries**: pandas, numpy, scipy\n",
                "- **Banking Applications**: Missing values, outliers, data quality\n",
                "- **Key Skills**: Imputation, outlier detection, data validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Cleaning Example\n",
                "import numpy as np\n",
                "from scipy import stats\n",
                "\n",
                "# Missing value handling\n",
                "# loan_data['income'].fillna(loan_data['income'].median(), inplace=True)\n",
                "\n",
                "# Outlier detection using IQR method\n",
                "def detect_outliers_iqr(data, column):\n",
                "    Q1 = data[column].quantile(0.25)\n",
                "    Q3 = data[column].quantile(0.75)\n",
                "    IQR = Q3 - Q1\n",
                "    lower_bound = Q1 - 1.5 * IQR\n",
                "    upper_bound = Q3 + 1.5 * IQR\n",
                "    return (data[column] < lower_bound) | (data[column] > upper_bound)\n",
                "\n",
                "print(\"Data cleaning functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Feature Engineering\n",
                "- **Key Libraries**: pandas, sklearn.preprocessing, category_encoders\n",
                "- **Banking Applications**: Payment patterns, behavioral variables, WOE/IV\n",
                "- **Key Skills**: Time-series features, encoding, transformation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Engineering Example\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "import category_encoders as ce\n",
                "\n",
                "# Weight of Evidence calculation\n",
                "def calculate_woe_iv(data, feature, target):\n",
                "    \"\"\"\n",
                "    Calculate Weight of Evidence and Information Value\n",
                "    \"\"\"\n",
                "    df = data.groupby(feature).agg({target: ['count', 'sum']}).reset_index()\n",
                "    df.columns = [feature, 'total', 'bad']\n",
                "    df['good'] = df['total'] - df['bad']\n",
                "    \n",
                "    total_good = df['good'].sum()\n",
                "    total_bad = df['bad'].sum()\n",
                "    \n",
                "    df['good_rate'] = df['good'] / total_good\n",
                "    df['bad_rate'] = df['bad'] / total_bad\n",
                "    df['woe'] = np.log(df['good_rate'] / df['bad_rate'])\n",
                "    df['iv'] = (df['good_rate'] - df['bad_rate']) * df['woe']\n",
                "    \n",
                "    return df, df['iv'].sum()\n",
                "\n",
                "print(\"Feature engineering functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Analysis\n",
                "\n",
                "### Exploratory Analysis\n",
                "- **Key Libraries**: pandas, matplotlib, seaborn\n",
                "- **Banking Applications**: Portfolio segmentation, risk patterns, correlation analysis\n",
                "- **Key Skills**: Statistical analysis, trend identification, data storytelling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exploratory Analysis Example\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set plotting style\n",
                "plt.style.use('default')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "# Example EDA functions\n",
                "def plot_default_rates_by_segment(data, segment_col, target_col):\n",
                "    \"\"\"\n",
                "    Plot default rates by customer segment\n",
                "    \"\"\"\n",
                "    default_rates = data.groupby(segment_col)[target_col].mean()\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    default_rates.plot(kind='bar')\n",
                "    plt.title(f'Default Rates by {segment_col}')\n",
                "    plt.ylabel('Default Rate')\n",
                "    plt.xticks(rotation=45)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "def correlation_heatmap(data, features):\n",
                "    \"\"\"\n",
                "    Create correlation heatmap for risk factors\n",
                "    \"\"\"\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    sns.heatmap(data[features].corr(), annot=True, cmap='coolwarm', center=0)\n",
                "    plt.title('Risk Factor Correlation Matrix')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "print(\"EDA functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Credit Modeling\n",
                "- **Key Libraries**: sklearn, statsmodels, xgboost\n",
                "- **Banking Applications**: PD models, credit scoring, risk classification\n",
                "- **Key Skills**: Logistic regression, ensemble methods, hyperparameter tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Credit Modeling Example\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.pipeline import Pipeline\n",
                "import xgboost as xgb\n",
                "\n",
                "# Logistic Regression for PD modeling\n",
                "def build_logistic_regression_model(X, y):\n",
                "    \"\"\"\n",
                "    Build and train logistic regression model for PD\n",
                "    \"\"\"\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "    \n",
                "    # Create pipeline with scaling and logistic regression\n",
                "    pipeline = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('logistic', LogisticRegression(random_state=42))\n",
                "    ])\n",
                "    \n",
                "    # Fit model\n",
                "    pipeline.fit(X_train, y_train)\n",
                "    \n",
                "    return pipeline, X_test, y_test\n",
                "\n",
                "# XGBoost model\n",
                "def build_xgboost_model(X, y):\n",
                "    \"\"\"\n",
                "    Build XGBoost model for credit scoring\n",
                "    \"\"\"\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "    \n",
                "    model = xgb.XGBClassifier(\n",
                "        objective='binary:logistic',\n",
                "        random_state=42,\n",
                "        eval_metric='auc'\n",
                "    )\n",
                "    \n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    return model, X_test, y_test\n",
                "\n",
                "print(\"Credit modeling functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Validation & Reporting\n",
                "- **Key Libraries**: sklearn.metrics, plotly, dash\n",
                "- **Banking Applications**: Model performance, regulatory reports, executive dashboards\n",
                "- **Key Skills**: ROC/AUC, backtesting, interactive visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Validation Example\n",
                "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
                "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
                "import plotly.graph_objects as go\n",
                "import plotly.express as px\n",
                "\n",
                "def evaluate_credit_model(model, X_test, y_test):\n",
                "    \"\"\"\n",
                "    Comprehensive model evaluation for credit risk\n",
                "    \"\"\"\n",
                "    # Predictions\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # AUC Score\n",
                "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
                "    \n",
                "    # Gini Coefficient\n",
                "    gini = 2 * auc_score - 1\n",
                "    \n",
                "    print(f\"AUC Score: {auc_score:.4f}\")\n",
                "    print(f\"Gini Coefficient: {gini:.4f}\")\n",
                "    \n",
                "    return {\n",
                "        'auc': auc_score,\n",
                "        'gini': gini,\n",
                "        'predictions': y_pred,\n",
                "        'probabilities': y_pred_proba\n",
                "    }\n",
                "\n",
                "def plot_roc_curve(y_test, y_pred_proba):\n",
                "    \"\"\"\n",
                "    Plot ROC curve using Plotly\n",
                "    \"\"\"\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
                "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
                "    \n",
                "    fig = go.Figure()\n",
                "    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=f'ROC Curve (AUC = {auc_score:.3f})'))\n",
                "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random Classifier'))\n",
                "    \n",
                "    fig.update_layout(\n",
                "        title='ROC Curve - Credit Risk Model',\n",
                "        xaxis_title='False Positive Rate',\n",
                "        yaxis_title='True Positive Rate'\n",
                "    )\n",
                "    \n",
                "    return fig\n",
                "\n",
                "def ks_statistic(y_test, y_pred_proba):\n",
                "    \"\"\"\n",
                "    Calculate Kolmogorov-Smirnov statistic\n",
                "    \"\"\"\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
                "    ks = max(tpr - fpr)\n",
                "    return ks\n",
                "\n",
                "print(\"Model validation functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example Workflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example workflow implementation\n",
                "def credit_risk_modeling_workflow():\n",
                "    \"\"\"\n",
                "    Complete workflow for credit risk modeling\n",
                "    \"\"\"\n",
                "    print(\"Credit Risk Modeling Workflow:\")\n",
                "    print(\"1. Data Acquisition - Load loan portfolio data\")\n",
                "    print(\"2. Data Cleaning - Handle missing values and outliers\")\n",
                "    print(\"3. Feature Engineering - Create behavioral variables and WOE\")\n",
                "    print(\"4. Exploratory Analysis - Understand risk patterns\")\n",
                "    print(\"5. Model Building - Train logistic regression/XGBoost\")\n",
                "    print(\"6. Model Validation - Evaluate performance with AUC, Gini, KS\")\n",
                "    print(\"7. Reporting - Create dashboards and regulatory reports\")\n",
                "\n",
                "credit_risk_modeling_workflow()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **Load your actual credit data** into this framework\n",
                "2. **Customize the functions** based on your specific banking requirements\n",
                "3. **Implement regulatory requirements** (Basel III, IFRS 9)\n",
                "4. **Set up model monitoring** and challenger model processes\n",
                "5. **Create production pipelines** for real-time scoring\n",
                "\n",
                "This framework provides the foundation for implementing comprehensive credit risk models in a commercial banking environment."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}